{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Team -\n",
        "\n",
        "1. Ankit Kumar Aggarwal\n",
        "2. Avinash Rajaraman Swaminathan"
      ],
      "metadata": {
        "id": "P1Iy40A0l84O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Assignment 2 Naïve Bayes and Sentiment Classification and Logistic Regression\n",
        "Instructions\n",
        "* Read the following Chapter 4: Naive Bayes and Sentiment Classification. Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright © 2021. All rights reserved. Draft of September 21, 2021. I have tried to pull out relevant notes for you below, but it is encouraged that you read each chapter provided.\n",
        "* Read the following Chapter 5: Logistic Regression. Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright © 2021. All rights reserved. Draft of September 21, 2021. I have tried to pull out relevant notes for you below, but it is encouraged that you read each chapter provided.\n",
        "\n",
        "Summary\n",
        "Classification is one of the most important tasks of NLP and in machine learning. In NLP it often means the task of text categorization for both sentiment analysis, spam detection, and topic modeling. Naïve Bayes is often one of the first classification algorithms defined in NLP.  The intuition behind a classifier is lies at the underlying probability inferred by the Bayesian Inference, which uses Baye’s rule and conditional probabilities.\n",
        "\n",
        "Here’s a reminder on Baye’s Rule:\n",
        "P(y)=P(x)P(x)/(P(y))\n",
        "\n",
        "We are saying “what is the probability of x given y”. Naïve Bayes is a generative model because there is an input that helps the model determine what the output could be. Said differently, “to train a generative model we first collect a large amount of data in some domain (e.g., think millions of images, sentences, or sounds, etc.) and then train a model to generate data like it.” [6]\n",
        "\n",
        "So in the case of Naïve Bayes, we say given some word, what should be the class of the current word we are assessing? Contrastingly, discriminative models such as logistic regression, learn from features provided to the algorithm and then determine or predict what the class is. [7]\n",
        "\n",
        "\n",
        "With Naïve Bayes, the assumption is that the probabilities are independent. We often call the Naïve Bayes classifier the bag-of-words approach. That’s because we are essentially throwing in the collection of words into a ‘bag’, selecting a word at random, and then calculating their frequency to use in the Bayesian Inference. Thus, context – the position of words -- is ignored and despite this, it turns out that the Naïve Bayes approach can be accurate and effective at determining whether an email is spam for example.\n",
        "\n",
        "Back to bag of words. With bag of words, we assume that the position of the words are not relevant -- that dependency or context in the word phrase or sentence doesn’t matter. Relatedly, the naive Bayes assumption implies that the conditional probabilities are independent -- a rather strange assumption to make for words in a sentence! The equation for the naive Bayes classifier is outlined below:\n",
        "\n",
        "You can use Naive Bayes by creating an index of words and walking through every word position in a test or corpus.\n"
      ],
      "metadata": {
        "id": "liqKR9Vk9RSj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It can be useful to be able to classify new \"test\" documents using already classified \"training\" documents.  A common example is using a corpus of labeled spam and ham (non-spam) e-mails to predict whether or not a new document is spam.\n",
        "\n",
        "For this Assignment, you can start with a spam/ham dataset, then predict the class of new documents (either withheld from the training dataset or from another source such as your own spam folder).   One example corpus:   https://spamassassin.apache.org/old/publiccorpus/\n",
        "\n",
        "You may work alone or in a group on this project.  You're welcome to use any tools or approach that you like.  Due before our next meetup. Starter code provided below.\n",
        "\n",
        "Test example is provided at the end."
      ],
      "metadata": {
        "id": "CIBB2IVT92Ed"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Libraries you may wish to use"
      ],
      "metadata": {
        "id": "c8sZQL-a-cHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from os import makedirs, path, remove, rename, rmdir\n",
        "from tarfile import open as open_tar\n",
        "from shutil import rmtree\n",
        "from urllib import request, parse\n",
        "from glob import glob\n",
        "from os import path\n",
        "from re import sub\n",
        "from email import message_from_file\n",
        "from glob import glob\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from collections import defaultdict\n",
        "from functools import partial\n",
        "from sklearn.metrics import (accuracy_score, f1_score, precision_score, recall_score)\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import gc"
      ],
      "metadata": {
        "id": "NHiCf9fi9103"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "download corpus using the following functions\n",
        "\n",
        "Note: you may need to mount your drive on google then run this location. See previous exercises."
      ],
      "metadata": {
        "id": "uObO057u-Rne"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4fIGS9-8wce"
      },
      "outputs": [],
      "source": [
        "def download_corpus(dataset_dir: str = 'data'):\n",
        "    base_url = 'https://spamassassin.apache.org'\n",
        "    corpus_path = 'old/publiccorpus'\n",
        "    files = {\n",
        "        '20021010_easy_ham.tar.bz2': 'ham',\n",
        "        '20021010_hard_ham.tar.bz2': 'ham',\n",
        "        '20021010_spam.tar.bz2': 'spam',\n",
        "        '20030228_easy_ham.tar.bz2': 'ham',\n",
        "        '20030228_easy_ham_2.tar.bz2': 'ham',\n",
        "        '20030228_hard_ham.tar.bz2': 'ham',\n",
        "        '20030228_spam.tar.bz2': 'spam',\n",
        "        '20030228_spam_2.tar.bz2': 'spam',\n",
        "        '20050311_spam_2.tar.bz2': 'spam' }\n",
        "\n",
        "    #creates the folders: downloads, ham and spam\n",
        "    downloads_dir = path.join(dataset_dir, 'downloads')\n",
        "    ham_dir = path.join(dataset_dir, 'ham')\n",
        "    spam_dir = path.join(dataset_dir, 'spam')\n",
        "\n",
        "    makedirs(downloads_dir, exist_ok=True)\n",
        "    makedirs(ham_dir, exist_ok=True)\n",
        "    makedirs(spam_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "    for file, spam_or_ham in files.items():\n",
        "        # download files from URL of each specific .bz2 file\n",
        "        url = parse.urljoin(base_url, f'{corpus_path}/{file}')\n",
        "        tar_filename = path.join(downloads_dir, file)\n",
        "        request.urlretrieve(url, tar_filename)\n",
        "\n",
        "        #list e-mails in the compressed .bz2 file\n",
        "        emails = []\n",
        "        with open_tar(tar_filename) as tar:\n",
        "            tar.extractall(path=downloads_dir)\n",
        "            for tarinfo in tar:\n",
        "                if len(tarinfo.name.split('/')) > 1:\n",
        "                    emails.append(tarinfo.name)\n",
        "\n",
        "        # move e-mails to ham or spam directory\n",
        "        for email in emails:\n",
        "            directory, filename = email.split('/')\n",
        "            directory = path.join(downloads_dir, directory)\n",
        "\n",
        "            if not path.exists(path.join(dataset_dir, spam_or_ham, filename)):\n",
        "                rename(path.join(directory, filename),\n",
        "                   path.join(dataset_dir, spam_or_ham, filename))\n",
        "\n",
        "        rmtree(directory)\n",
        "\n",
        "download_corpus()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#How many e-mails are classified in our dataset as either Spam or not Spam?\n"
      ],
      "metadata": {
        "id": "MUmHvbCn-o3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#How many e-mails are classified in our dataset as either Spam or not Spam?\n",
        "ham_dir = path.join('data', 'ham')\n",
        "spam_dir = path.join('data', 'spam')\n",
        "\n",
        "print('Number of Non-Spam E-mails:', len(glob(f'{ham_dir}/*')))\n",
        "print('\\nNumber of Spam E-mails:', len(glob(f'{spam_dir}/*')))"
      ],
      "metadata": {
        "id": "Cx-Blo33-oM1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eda7673c-cf05-4c5f-fa16-b8813b353515"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Non-Spam E-mails: 6952\n",
            "\n",
            "Number of Spam E-mails: 2399\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing the text"
      ],
      "metadata": {
        "id": "Xo4dguod0qL7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import make_pipeline"
      ],
      "metadata": {
        "id": "FmU_KeZM0pjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract email text\n",
        "def extract_email_text(file_path):\n",
        "    with open(file_path, 'r', errors='ignore') as file:\n",
        "        email_message = message_from_file(file)\n",
        "        email_body = email_message.get_payload()\n",
        "\n",
        "        # Checking if the email body is a list (for multipart messages)\n",
        "        if isinstance(email_body, list):\n",
        "            # Getting only the plaintext part\n",
        "            email_body = [part for part in email_body if part.get_content_type() == 'text/plain']\n",
        "\n",
        "            # If plaintext part found, take it, else return an empty string\n",
        "            if email_body:\n",
        "                email_body = email_body[0].get_payload()\n",
        "            else:\n",
        "                email_body = ''\n",
        "    return email_body\n",
        "\n",
        "def get_emails(directory):\n",
        "    file_paths = glob(f'{directory}/*')  # get paths of all files in the directory\n",
        "    emails = [extract_email_text(file_path) for file_path in file_paths]  # apply the function to each file\n",
        "    return emails\n",
        "\n",
        "spam_emails = get_emails(spam_dir)\n",
        "ham_emails = get_emails(ham_dir)\n",
        "\n",
        "# Combining all emails and creating corresponding labels (Spam = 1, Ham = 0)\n",
        "all_emails = spam_emails + ham_emails\n",
        "email_labels = [1] * len(spam_emails) + [0] * len(ham_emails)\n",
        "\n",
        "# Vectorizing the email text\n",
        "email_vectorizer = CountVectorizer()\n",
        "email_vectors = email_vectorizer.fit_transform(all_emails)"
      ],
      "metadata": {
        "id": "PV653AgX0uCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train-Test Split"
      ],
      "metadata": {
        "id": "6IJxLm0b2EWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Splitting the data into training and testing datasets\n",
        "X_train, X_test, y_train, y_test = train_test_split(email_vectors, email_labels, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "_NXwivzS0uE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your classifier below"
      ],
      "metadata": {
        "id": "v3fSuJ0G_jNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##### START CODE ####\n",
        "\n",
        "\"\"\" Enter code here\"\"\"\n",
        "\n",
        "# Training a Naive Bayes classifier\n",
        "nb_classifier = MultinomialNB()\n",
        "nb_classifier.fit(X_train, y_train)\n",
        "\n",
        "#### END CODE ####"
      ],
      "metadata": {
        "id": "MO9eKbq8_llJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "cfc1f6b0-3e43-4be7-f6dd-40c4ebda73cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB()"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "predicted_labels = nb_classifier.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, predicted_labels, target_names=['Ham', 'Spam']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VA5J-sv41zOb",
        "outputId": "eb10ae37-a73f-4705-c378-7dfd6341c529"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         Ham       0.97      1.00      0.98      1390\n",
            "        Spam       1.00      0.90      0.95       481\n",
            "\n",
            "    accuracy                           0.97      1871\n",
            "   macro avg       0.98      0.95      0.96      1871\n",
            "weighted avg       0.97      0.97      0.97      1871\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification Report Inference\n",
        "\n",
        "The model appears to be very effective at identifying 'Ham' emails but not good at identifying 'Spam' emails.\n",
        "\n",
        "**When it identifies an email as \"Spam\", it is almost seems to be correct. The lower recall for 'Spam' suggests that there are some spam emails that the model is not able to correctly identify. These would be false negatives, or spam emails that are getting through the filter**"
      ],
      "metadata": {
        "id": "Ccqrom4B3YB1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following email is a test email. You can take this and test your classifier to see if it predicts spam or not.\n",
        "\n"
      ],
      "metadata": {
        "id": "J5WtArb0_IMk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test a sample"
      ],
      "metadata": {
        "id": "djGX48ly14Yq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_document(doc):\n",
        "    doc = email_vectorizer.transform([doc])  # change 'vectorizer' to 'email_vectorizer'\n",
        "    return 'Spam' if nb_classifier.predict(doc) == 1 else 'Ham'\n",
        "\n",
        "# Test on a new document\n",
        "spam_email = \"\"\"\n",
        "Subject: Get Rich Quick!\n",
        "\n",
        "Dear Friend,\n",
        "\n",
        "Congratulations! You've been selected to participate in an exclusive opportunity to make thousands of dollars from the comfort of your own home. Our revolutionary system guarantees quick and easy cash with minimal effort.\n",
        "\n",
        "No more struggling to pay bills or worrying about financial security. With our proven method, you can start earning massive amounts of money in no time.\n",
        "\n",
        "Here's what some of our satisfied customers have to say:\n",
        "- \"I was skeptical at first, but I'm now living my dream life thanks to this incredible system!\" - John S.\n",
        "- \"I never thought making money online could be this simple. It's changed my life!\" - Sarah L.\n",
        "\n",
        "Don't miss out on this limited-time offer. Act now to secure your spot and start enjoying a life of financial freedom.\n",
        "\n",
        "Click the link below to get started:\n",
        "www.getrichquick.com\n",
        "\n",
        "Remember, this opportunity is exclusive and won't last long. Take control of your financial future today!\n",
        "\n",
        "Best regards,\n",
        "The Get Rich Quick Team\n",
        "\"\"\"\n",
        "print(classify_document(spam_email))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWdYGnbe13yR",
        "outputId": "89eab0c4-5431-4e01-c7b3-dc2ae3aea37d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spam\n"
          ]
        }
      ]
    }
  ]
}